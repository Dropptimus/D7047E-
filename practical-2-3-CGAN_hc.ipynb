{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"\\data\", \".\")\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = PATH_DATASETS,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        num_workers: int = NUM_WORKERS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.13,), (0.31,)), #slight deviation from tutorial\n",
    "            ]\n",
    "        )\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(self.data_dir, train = True, download = True)\n",
    "        MNIST(self.data_dir, train = False, download = True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train = True, transform = self.transform)\n",
    "            val_size = int(len(mnist_full) * 0.1)\n",
    "            train_size = len(mnist_full) - val_size\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [54016, 5984]) #This split because % 64 = 0\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.minst_test = MNIST(self.data_dir, train = False, transform = self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size = self.batch_size, num_workers = self.num_workers)\n",
    "\n",
    "    # Gan does not use validation set but included for learning lightning purposes\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size = self.batch_size, num_workers = self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size = self.batch_size, num_workers = self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape, classes):\n",
    "        super().__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.classes = classes\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self._create_layer(latent_dim + self.classes, 128, False),\n",
    "            *self._create_layer(128, 256),\n",
    "            *self._create_layer(256, 512),\n",
    "            *self._create_layer(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _create_layer(self, size_in, size_out, normalize=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        input = torch.cat((self.label_embedding(c), z), -1)\n",
    "        img = self.model(input)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape, classes):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self._create_layer(self.classes + int(np.prod(img_shape)), 2048, False, True),\n",
    "            *self._create_layer(2048, 1024, True, True),\n",
    "            *self._create_layer(1024, 512, True, True),\n",
    "            *self._create_layer(512, 256, True, True),\n",
    "            *self._create_layer(256, 128, False, False),\n",
    "            *self._create_layer(128, 1, False, False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _create_layer(self, size_in, size_out, drop_out=True, act_func=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(0.6)) \n",
    "        if act_func:\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    #Increased dropout 0.4 -> 0.6 and added a 2048 -> 1024 layer seem to reduce noise but increase mode collapse\n",
    "    #Added 4096 -> 2048 layer reduced surrounding noise even more but sometimes created inverted images\n",
    "    #3096 -> 2048 layer just bad\n",
    "    \n",
    "    def forward(self, img, c):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        input = torch.cat((img_flat, self.label_embedding(c)), -1)\n",
    "        validity = self.model(input)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        channels, \n",
    "        width, \n",
    "        height, \n",
    "        latent_dim: int = 100, \n",
    "        lr: float = 1e-3,\n",
    "        b1: float = 0.5,\n",
    "        b2: float = 0.999,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        loss_type: str = \"BCE\",\n",
    "        classes: int = 10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        data_shape = (channels, width, height)\n",
    "        self.generator = Generator(latent_dim=self.hparams.latent_dim, img_shape=data_shape, classes=classes)\n",
    "        self.discriminator = Discriminator(img_shape=data_shape, classes=classes)\n",
    "\n",
    "        self.validation_z = torch.randn(8, self.hparams.latent_dim)\n",
    "        \n",
    "        self.loss_type = loss_type\n",
    "        self.classes = classes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        return self.generator(z, c)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        imgs, labels = batch\n",
    "        optimizer_g, optimizer_d = self.optimizers()\n",
    "\n",
    "        # create noise\n",
    "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\n",
    "        z = z.type_as(imgs)\n",
    "        fake_labels = torch.randint(0, self.classes, (self.batch_size,), device=device)\n",
    "        fake_labels = torch.full_like(fake_labels, fill_value=5, device=device)\n",
    "        \n",
    "        #train generator and generate images\n",
    "        self.toggle_optimizer(optimizer_g)\n",
    "        self.generated_imgs = self(z, fake_labels)\n",
    "\n",
    "\n",
    "        # add sampled images to log\n",
    "        sample_imgs = self.generated_imgs[:6]\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)\n",
    "\n",
    "        # ground truth result (ie: all fake)\n",
    "        # put on GPU because we created this tensor inside training_loop\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs)\n",
    "\n",
    "        # adversarial loss is binary cross-entropy\n",
    "        if self.loss_type == \"vanilla\":\n",
    "            g_loss = -torch.mean(torch.log(self.discriminator(self(z, fake_labels), fake_labels)))\n",
    "        else:\n",
    "            g_loss = self.adversarial_loss(self.discriminator(self(z, fake_labels), fake_labels), valid)\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "        self.manual_backward(g_loss)\n",
    "        optimizer_g.step()\n",
    "        optimizer_g.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_g)\n",
    "\n",
    "        # train discriminator\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        self.toggle_optimizer(optimizer_d)\n",
    "\n",
    "        # how well can it label as real?\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs)\n",
    "\n",
    "        real_loss = self.adversarial_loss(self.discriminator(imgs, labels), valid)\n",
    "\n",
    "        # how well can it label as fake?\n",
    "        fake = torch.zeros(imgs.size(0), 1)\n",
    "        fake = fake.type_as(imgs)\n",
    "\n",
    "        fake_loss = self.adversarial_loss(self.discriminator(self(z, fake_labels).detach(), fake_labels), fake)\n",
    "\n",
    "        if self.loss_type == \"vanilla\":\n",
    "            d_loss = -torch.mean(torch.log(self.discriminator(imgs, labels)) + torch.log(1. - fake))\n",
    "        else:\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "        self.manual_backward(d_loss)\n",
    "        optimizer_d.step()\n",
    "        optimizer_d.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_d)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        # Origninal vanilla implemtaton did not set ADAM betas so skip that here\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        # opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr)\n",
    "        # opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "        return [opt_g, opt_d], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 1.5 M \n",
      "1 | discriminator | Discriminator | 14.4 M\n",
      "------------------------------------------------\n",
      "15.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.9 M    Total params\n",
      "63.783    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/844 [97:42:10<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:40:16<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:32:28<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:31:32<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:31:04<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:30:47<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:30:09<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:29:33<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:28:54<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:28:41<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [97:24:17<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/844 [96:47:00<?, ?it/s]\n",
      "Epoch 4: 100%|██████████| 844/844 [00:40<00:00, 20.63it/s, v_num=140, g_loss=37.50, d_loss=31.20]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 844/844 [00:41<00:00, 20.53it/s, v_num=140, g_loss=37.50, d_loss=31.20]\n"
     ]
    }
   ],
   "source": [
    "dm = MNISTDataModule()\n",
    "model = GAN(*dm.dims, loss_type = \"BCE\")\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=5,\n",
    ")\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 68702), started 2 days, 18:34:53 ago. (Use '!kill 68702' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-25457656068c2519\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-25457656068c2519\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnlm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
